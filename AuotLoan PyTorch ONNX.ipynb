{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "project_dir = '/sasinside/userdata/gegrab/resources/hmeq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.   Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loan_ID  LOAN  LOANDUE     VALUE  REASON     JOB   YOJ  DEROG  DELINQ  \\\n",
      "0   772418  1100  25860.0   39025.0  CarImp   Other  10.5    0.0     0.0   \n",
      "1   477724  1300  70053.0   68400.0  CarImp   Other   7.0    0.0     2.0   \n",
      "2   150746  1500  13500.0   16700.0  CarImp   Other   4.0    0.0     0.0   \n",
      "3   108584  1500      NaN       NaN     NaN     NaN   NaN    NaN     NaN   \n",
      "4   321534  1700  97800.0  112000.0  CarImp  Office   3.0    0.0     0.0   \n",
      "\n",
      "        CLAGE  NINQ  CLNO  DEBTINC  \n",
      "0   94.366667   1.0   9.0      NaN  \n",
      "1  121.833333   0.0  14.0      NaN  \n",
      "2  149.466667   1.0  10.0      NaN  \n",
      "3         NaN   NaN   NaN      NaN  \n",
      "4   93.333333   0.0  14.0      NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data_orig/AutoLoan.csv')\n",
    "#df = df.dropna() # Or some way to treat missing values \n",
    "\n",
    "auto= df.drop('BAD', axis=1)\n",
    "\n",
    " \n",
    " \n",
    "class_inputs   = ['REASON', 'JOB']\n",
    "target         = [\"BAD\"]\n",
    "numeric_inputs = ['LOAN', 'LOANDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ','CLNO', 'DEBTINC']\n",
    "class_inputs   = ['REASON', 'JOB']\n",
    "\n",
    "impute_values = df[numeric_inputs].mean()\n",
    "pickle.dump(impute_values, open('/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_impute.pickle','wb'))\n",
    "   \n",
    "    \n",
    "df           =df.fillna(impute_values)\n",
    "\n",
    " \n",
    "df.REASON.replace(np.nan,'CarImp',regex = True, inplace=True)\n",
    "df.JOB.replace(np.nan,'Other',regex = True, inplace=True)\n",
    "\n",
    "sample =auto.head()\n",
    "\n",
    " \n",
    "sample.to_csv('Data_orig/autoloan2_pytorch_test2.csv', index=False)\n",
    " \n",
    "print(sample)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='BAD', ylabel='count'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'BAD', data=df)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Create Input and Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: BAD, dtype: int64\n",
      "   LOAN     LOANDUE          VALUE  REASON     JOB        YOJ    DEROG  \\\n",
      "0  1100  25860.0000   39025.000000  CarImp   Other  10.500000  0.00000   \n",
      "1  1300  70053.0000   68400.000000  CarImp   Other   7.000000  0.00000   \n",
      "2  1500  13500.0000   16700.000000  CarImp   Other   4.000000  0.00000   \n",
      "3  1500  73760.8172  101776.048741  CarImp   Other   8.922268  0.25457   \n",
      "4  1700  97800.0000  112000.000000  CarImp  Office   3.000000  0.00000   \n",
      "\n",
      "     DELINQ       CLAGE      NINQ       CLNO    DEBTINC  \n",
      "0  0.000000   94.366667  1.000000   9.000000  33.779915  \n",
      "1  2.000000  121.833333  0.000000  14.000000  33.779915  \n",
      "2  0.000000  149.466667  1.000000  10.000000  33.779915  \n",
      "3  0.449442  179.766275  1.186055  21.296096  33.779915  \n",
      "4  0.000000   93.333333  0.000000  14.000000  33.779915  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = df.iloc[:,1]\n",
    "X = df.iloc[:,2:] \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "print(y.head())\n",
    "print(X.head())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1549\n",
      "1     418\n",
      "Name: BAD, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "\n",
    "count = y_test.value_counts()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/4114303409.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X_train.drop(class_inputs,1,inplace=True)\n",
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/4114303409.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X_test.drop(class_inputs,1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse = False, handle_unknown=\"ignore\")\n",
    "enc =  ohe.fit_transform(X_train[class_inputs])\n",
    "\n",
    "X_train[ohe.get_feature_names(class_inputs)] = pd.DataFrame(enc, index=X_train.index)\n",
    "X_train.drop(class_inputs,1,inplace=True)\n",
    "#print(X_train.head())\n",
    "\n",
    "enc2 =  ohe.fit_transform(X_test[class_inputs])\n",
    "X_test[ohe.get_feature_names(class_inputs)] = pd.DataFrame(enc2, index=X_test.index)\n",
    "X_test.drop(class_inputs,1,inplace=True)\n",
    "#print(X_test.head())\n",
    " \n",
    "pickle.dump(ohe, open(project_dir+\"/autoloan_pytorch_encoder.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test.head())\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "# fit scaler on the training dataset\n",
    "#scaler.fit(X_train)\n",
    "#X_train_scaled = scaler.transform(X_train)\n",
    "pickle.dump(scaler, open('/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_scaler.pickle', 'wb'))\n",
    "\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.  Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.  Define Custom Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "#test_tensor = torch.Tensor(test.values)\n",
    "train_data = trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train.values))\n",
    "                       #torch.Float(y_train))\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.  Define Neural Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 18.      \n",
    "        self.layer_1 = nn.Linear(18, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "         \n",
    "       \n",
    "        return x\n",
    "    \n",
    "net = binaryClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "###################### OUTPUT ######################\n",
    "cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=18, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = binaryClassification()\n",
    "model2.to(device)\n",
    "print(model2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.57487 | Acc: 73.889\n",
      "Epoch 002: | Loss: 0.45625 | Acc: 82.651\n",
      "Epoch 003: | Loss: 0.37601 | Acc: 86.111\n",
      "Epoch 004: | Loss: 0.32615 | Acc: 87.413\n",
      "Epoch 005: | Loss: 0.30044 | Acc: 88.206\n",
      "Epoch 006: | Loss: 0.28114 | Acc: 89.079\n",
      "Epoch 007: | Loss: 0.26410 | Acc: 89.619\n",
      "Epoch 008: | Loss: 0.25018 | Acc: 90.159\n",
      "Epoch 009: | Loss: 0.23599 | Acc: 90.825\n",
      "Epoch 010: | Loss: 0.22060 | Acc: 91.571\n",
      "Epoch 011: | Loss: 0.20948 | Acc: 92.270\n",
      "Epoch 012: | Loss: 0.19517 | Acc: 92.952\n",
      "Epoch 013: | Loss: 0.18319 | Acc: 93.206\n",
      "Epoch 014: | Loss: 0.17230 | Acc: 93.968\n",
      "Epoch 015: | Loss: 0.16184 | Acc: 94.571\n",
      "Epoch 016: | Loss: 0.15295 | Acc: 94.619\n",
      "Epoch 017: | Loss: 0.14158 | Acc: 95.286\n",
      "Epoch 018: | Loss: 0.13561 | Acc: 95.270\n",
      "Epoch 019: | Loss: 0.12783 | Acc: 95.794\n",
      "Epoch 020: | Loss: 0.12679 | Acc: 95.349\n",
      "Epoch 021: | Loss: 0.11415 | Acc: 96.190\n",
      "Epoch 022: | Loss: 0.11048 | Acc: 96.000\n",
      "Epoch 023: | Loss: 0.10598 | Acc: 96.222\n",
      "Epoch 024: | Loss: 0.09771 | Acc: 96.556\n",
      "Epoch 025: | Loss: 0.08900 | Acc: 96.698\n",
      "Epoch 026: | Loss: 0.09311 | Acc: 96.286\n",
      "Epoch 027: | Loss: 0.08610 | Acc: 96.889\n",
      "Epoch 028: | Loss: 0.07983 | Acc: 97.111\n",
      "Epoch 029: | Loss: 0.07770 | Acc: 97.143\n",
      "Epoch 030: | Loss: 0.07077 | Acc: 97.730\n",
      "Epoch 031: | Loss: 0.07555 | Acc: 97.254\n",
      "Epoch 032: | Loss: 0.06109 | Acc: 97.905\n",
      "Epoch 033: | Loss: 0.05903 | Acc: 97.937\n",
      "Epoch 034: | Loss: 0.05890 | Acc: 98.206\n",
      "Epoch 035: | Loss: 0.05412 | Acc: 98.063\n",
      "Epoch 036: | Loss: 0.05313 | Acc: 98.286\n",
      "Epoch 037: | Loss: 0.05699 | Acc: 98.079\n",
      "Epoch 038: | Loss: 0.05694 | Acc: 97.683\n",
      "Epoch 039: | Loss: 0.05993 | Acc: 97.730\n",
      "Epoch 040: | Loss: 0.04923 | Acc: 98.175\n",
      "Epoch 041: | Loss: 0.04553 | Acc: 98.429\n",
      "Epoch 042: | Loss: 0.03831 | Acc: 98.825\n",
      "Epoch 043: | Loss: 0.04694 | Acc: 98.317\n",
      "Epoch 044: | Loss: 0.04203 | Acc: 98.429\n",
      "Epoch 045: | Loss: 0.04844 | Acc: 98.238\n",
      "Epoch 046: | Loss: 0.04681 | Acc: 97.984\n",
      "Epoch 047: | Loss: 0.04032 | Acc: 98.429\n",
      "Epoch 048: | Loss: 0.04154 | Acc: 98.508\n",
      "Epoch 049: | Loss: 0.03723 | Acc: 98.730\n",
      "Epoch 050: | Loss: 0.03412 | Acc: 98.810\n"
     ]
    }
   ],
   "source": [
    "model2.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model2(X_batch)\n",
    "      \n",
    "        gene = torch.sigmoid(y_pred) \n",
    "        #gene = y_pred\n",
    "        px = pd.DataFrame(gene).astype(\"float\")\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.  Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.6770942]], dtype=float32), array([[0.00026256]], dtype=float32), array([[1.]], dtype=float32), array([[7.4993826e-05]], dtype=float32), array([[0.00051749]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "gene_list =[]\n",
    "y_test_pred_list= [] \n",
    "input_list = []\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model2(X_batch)\n",
    "        input = X_batch\n",
    "        gene = model2(X_batch)   \n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        gene = torch.sigmoid(gene) \n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        \n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        gene_list.append(gene.cpu().numpy())\n",
    "        y_test_pred_list.append(y_test_pred.cpu().numpy())\n",
    "        input_list.append(input.cpu().numpy())\n",
    "\n",
    " \n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "\n",
    "gene_list = [a.squeeze().tolist() for a in gene_list]\n",
    "                          \n",
    "input_list =[a.squeeze().tolist() for a in input_list]                  \n",
    "    \n",
    "\n",
    "\n",
    "#print(input_list[0])\n",
    "#print(gene[:10])\n",
    " \n",
    "print(y_test_pred_list[:5])\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.  Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1519,   30],\n",
       "       [ 174,  244]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.  Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      1549\n",
      "           1       0.89      0.58      0.71       418\n",
      "\n",
      "    accuracy                           0.90      1967\n",
      "   macro avg       0.89      0.78      0.82      1967\n",
      "weighted avg       0.90      0.90      0.89      1967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.  Save and Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.677094\n",
      "1  0.000263\n",
      "2  1.000000\n",
      "3  0.000075\n",
      "4  0.000517\n"
     ]
    }
   ],
   "source": [
    "project_dir = '/sasinside/userdata/gegrab/resources/hmeq'\n",
    "torch.save(model2.state_dict(), project_dir +'/'+ 'AutoLoan_PyTorch_Classifier.pt')\n",
    "model3 = binaryClassification()\n",
    "model3.load_state_dict(torch.load(project_dir + '/' + 'AutoLoan_PyTorch_Classifier.pt'))\n",
    "model3.eval()\n",
    "\n",
    "scaler2 = pickle.load(open(project_dir + '/' + 'autoloan_pytorch_scaler.pickle', 'rb'))\n",
    "\n",
    "#inputMatrix = scaler2.transform(X_test)\n",
    "#already scaled!!!\n",
    "inputMatrix = X_test\n",
    "\n",
    "X_test_var = Variable(torch.FloatTensor(inputMatrix), requires_grad=True) \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predict_proba =model3(X_test_var)\n",
    "    predict_proba = torch.sigmoid(predict_proba)   \n",
    "    predict_proba = pd.DataFrame(predict_proba).astype(\"float\")\n",
    "    \n",
    "    \n",
    "print(predict_proba[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.677094\n",
      "1  0.000263\n",
      "2  1.000000\n",
      "3  0.000075\n",
      "4  0.000517\n",
      "          0\n",
      "0  0.677094\n",
      "1  0.000263\n",
      "2  1.000000\n",
      "3  0.000075\n",
      "4  0.000517\n"
     ]
    }
   ],
   "source": [
    "#pt vs pickle??\n",
    "\n",
    "project_dir = '/sasinside/userdata/gegrab/resources/hmeq'\n",
    "torch.save(model2.state_dict(), project_dir +'/'+ 'AutoLoan_PyTorch_Classifier.pt')\n",
    "model3 = binaryClassification()\n",
    "model3.load_state_dict(torch.load(project_dir + '/' + 'AutoLoan_PyTorch_Classifier.pt'))\n",
    "model3.eval()\n",
    "\n",
    "#original method\n",
    "torch.save(model2, project_dir +'/'+ 'AutoLoan_PyTorch_Classifier_orig.pt')\n",
    "model4=torch.load(project_dir +'/' + 'AutoLoan_PyTorch_Classifier_orig.pt')\n",
    "\n",
    "scaler2 = pickle.load(open(project_dir + '/' + 'autoloan_pytorch_scaler.pickle', 'rb'))\n",
    "\n",
    "\n",
    "modelx =  binaryClassification()\n",
    "modelx.load_state_dict(torch.load(\"/sasinside/userdata/gegrab/resources/hmeq/AutoLoan_PyTorch_Classifier.pt\"))\n",
    "modelx.eval()\n",
    "    \n",
    "\n",
    "#inputMatrix = scaler2.transform(X_test)\n",
    "#already scaled!!!\n",
    "inputMatrix = X_test\n",
    "\n",
    "X_test_var = Variable(torch.FloatTensor(inputMatrix), requires_grad=True) \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predict_proba =model3(X_test_var)\n",
    "    predict_proba = torch.sigmoid(predict_proba)   \n",
    "    predict_proba = pd.DataFrame(predict_proba).astype(\"float\")\n",
    "    \n",
    "    \n",
    "print(predict_proba[:5])\n",
    "\n",
    "with torch.no_grad():\n",
    "    predict_proba =model4(X_test_var)\n",
    "    predict_proba = torch.sigmoid(predict_proba)   \n",
    "    predict_proba = pd.DataFrame(predict_proba).astype(\"float\")\n",
    "    \n",
    "    \n",
    "print(predict_proba[:5])\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Loan_ID\", \"LOAN\", \"LOANDUE\", \"VALUE\", \"REASON\", \"JOB\", \"YOJ\", \"DEROG\", \"DELINQ\", \"CLAGE\", \"NINQ\", \"CLNO\", \"DEBTINC\"\n",
      "\n",
      "Loan_ID, LOAN, LOANDUE, VALUE, REASON, JOB, YOJ, DEROG, DELINQ, CLAGE, NINQ, CLNO, DEBTINC\n"
     ]
    }
   ],
   "source": [
    "input_params = ''\n",
    "for col in auto.columns:\n",
    "    input_params += col\n",
    "    if col != auto.columns[-1]:\n",
    "        input_params += ', '\n",
    "\n",
    "input_cols = ''\n",
    "for col in auto.columns:\n",
    "    input_cols += \"\\\"\" + col + \"\\\"\"\n",
    "    if col != auto.columns[-1]:\n",
    "        input_cols += ', '\n",
    "\n",
    "\n",
    "print(input_cols)\n",
    "print(\"\")\n",
    "print(input_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "\n",
    "\n",
    "import onnxruntime as rt\n",
    "import onnx\n",
    "torch.onnx.export(model2,               # model being run\n",
    "                  X_test_var[0:2],                         # model input (or a tuple for multiple inputs)\n",
    "                  \"/sasinside/userdata/gegrab/resources/hmeq/AutoLoan_PyTorch_ONNX.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to            \n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'] # the model's output names\n",
    "                 )\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(Loan_ID, LOAN, LOANDUE, VALUE, REASON, JOB, YOJ, DEROG, DELINQ, CLAGE, NINQ, CLNO, DEBTINC ):\n",
    "    \"Output: P_BAD\"\n",
    "        \n",
    "    #global pred_onnx\n",
    "    \n",
    "    import onnxruntime as rt\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.autograd import Variable\n",
    "     \n",
    "    \n",
    "    inputArray = pd.DataFrame([[Loan_ID, LOAN, LOANDUE, VALUE, REASON, JOB, YOJ, DEROG, DELINQ, CLAGE, NINQ, CLNO, DEBTINC]],\n",
    "                              columns = [\"Loan_ID\",\"LOAN\", \"LOANDUE\", \"VALUE\", \"REASON\", \"JOB\", \"YOJ\", \"DEROG\", \"DELINQ\", \"CLAGE\", \n",
    "                                         \"NINQ\", \"CLNO\", \"DEBTINC\"]\n",
    "                               )\n",
    "    \n",
    "    dummy=inputArray\n",
    "     \n",
    "    inputArray = pd.concat([inputArray,dummy], sort=False)\n",
    "    \n",
    "      \n",
    "    def preprocessing(df, ohe_loc = None):\n",
    "\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    \n",
    "        with open(ohe_loc, \"rb\") as ohe_file:\n",
    "            ohe = pickle.load(ohe_file) \n",
    "        \n",
    "        enc = ohe.transform(df[categorical_cols])\n",
    "    \n",
    "        df[ohe.get_feature_names(categorical_cols).tolist()] = pd.DataFrame(enc, index=df.index)\n",
    "\n",
    "        df.drop(categorical_cols,1,inplace=True)\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    \n",
    "    inputArray.fillna(pickle.load(open('/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_impute.pickle', 'rb')), inplace=True)\n",
    "    inputArray.REASON.replace(np.nan,'CarImp',regex = True, inplace=True)\n",
    "    inputArray.JOB.replace(np.nan,'Other',regex = True, inplace=True)\n",
    "\n",
    "    inputArray.drop(['Loan_ID'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    inputArray = preprocessing(inputArray, \"/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_encoder.pickle\")\n",
    "    \n",
    "     \n",
    "    \n",
    "    scaler2 = pickle.load(open(\"/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_scaler.pickle\", 'rb'))\n",
    "    \n",
    "    inputMatrix = scaler2.transform(inputArray)\n",
    "    \n",
    "     \n",
    "    \n",
    "    #X_test_var = Variable(torch.FloatTensor(inputMatrix), requires_grad=True) \n",
    "    X_test_var = torch.FloatTensor(inputMatrix)\n",
    "     \n",
    "\n",
    "\n",
    "    ort_session = rt.InferenceSession(\"/sasinside/userdata/gegrab/resources/hmeq/AutoLoan_PyTorch_ONNX.onnx\")\n",
    "\n",
    "    def to_numpy(tensor):\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(X_test_var)} \n",
    "    ort_outs = torch.FloatTensor(ort_session.run(None, ort_inputs))\n",
    "     \n",
    "     \n",
    "    predict_proba = torch.sigmoid(ort_outs)   \n",
    "    #predict_proba = predict_proba.numpy()\n",
    "    \n",
    "\n",
    "    P_BAD = float(predict_proba[0][0])\n",
    "     \n",
    "     \n",
    "    return (P_BAD)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999558925628662\n",
      "0.9984656572341919\n",
      "0.9999994039535522\n",
      "0.7779723405838013\n",
      "9.979824244510382e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/1900645834.py:39: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(categorical_cols,1,inplace=True)\n",
      "/tmp/ipykernel_25225/1900645834.py:74: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272204863/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  ort_outs = torch.FloatTensor(ort_session.run(None, ort_inputs))\n",
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/1900645834.py:39: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(categorical_cols,1,inplace=True)\n",
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/1900645834.py:39: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(categorical_cols,1,inplace=True)\n",
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/1900645834.py:39: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(categorical_cols,1,inplace=True)\n",
      "/sasinside/miniconda3/envs/pytorch2021/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/tmp/ipykernel_25225/1900645834.py:39: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(categorical_cols,1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import pandas as pd\n",
    "testdf= pd.read_csv('Data_orig/autoloan2_pytorch_test2.csv')  \n",
    " \n",
    "for i in range(5):\n",
    "    print(execute(**testdf.iloc[i]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_code = \"\"\"\n",
    "def execute(Loan_ID, LOAN, LOANDUE, VALUE, REASON, JOB, YOJ, DEROG, DELINQ, CLAGE, NINQ, CLNO, DEBTINC ):\n",
    "    \"Output: P_BAD\"\n",
    "        \n",
    "    #global pred_onnx\n",
    "    \n",
    "    import onnxruntime as rt\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.autograd import Variable\n",
    "     \n",
    "    \n",
    "    inputArray = pd.DataFrame([[Loan_ID, LOAN, LOANDUE, VALUE, REASON, JOB, YOJ, DEROG, DELINQ, CLAGE, NINQ, CLNO, DEBTINC]],\n",
    "                              columns = [\"Loan_ID\",\"LOAN\", \"LOANDUE\", \"VALUE\", \"REASON\", \"JOB\", \"YOJ\", \"DEROG\", \"DELINQ\", \"CLAGE\", \n",
    "                                         \"NINQ\", \"CLNO\", \"DEBTINC\"]\n",
    "                               )\n",
    "    \n",
    "    dummy=inputArray\n",
    "     \n",
    "    inputArray = pd.concat([inputArray,dummy], sort=False)\n",
    "    \n",
    "      \n",
    "    def preprocessing(df, ohe_loc = None):\n",
    "\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    \n",
    "        with open(ohe_loc, \"rb\") as ohe_file:\n",
    "            ohe = pickle.load(ohe_file) \n",
    "        \n",
    "        enc = ohe.transform(df[categorical_cols])\n",
    "    \n",
    "        df[ohe.get_feature_names(categorical_cols).tolist()] = pd.DataFrame(enc, index=df.index)\n",
    "\n",
    "        df.drop(categorical_cols,1,inplace=True)\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    \n",
    "    inputArray.fillna(pickle.load(open('/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_impute.pickle', 'rb')), inplace=True)\n",
    "    inputArray.REASON.replace(np.nan,'CarImp',regex = True, inplace=True)\n",
    "    inputArray.JOB.replace(np.nan,'Other',regex = True, inplace=True)\n",
    "\n",
    "    inputArray.drop(['Loan_ID'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    inputArray = preprocessing(inputArray, \"/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_encoder.pickle\")\n",
    "    \n",
    "     \n",
    "    \n",
    "    scaler2 = pickle.load(open(\"/sasinside/userdata/gegrab/resources/hmeq/autoloan_pytorch_scaler.pickle\", 'rb'))\n",
    "    \n",
    "    inputMatrix = scaler2.transform(inputArray)\n",
    "    \n",
    "     \n",
    "    \n",
    "    #X_test_var = Variable(torch.FloatTensor(inputMatrix), requires_grad=True) \n",
    "    X_test_var = torch.FloatTensor(inputMatrix)\n",
    "     \n",
    "\n",
    "\n",
    "    ort_session = rt.InferenceSession(\"/sasinside/userdata/gegrab/resources/hmeq/AutoLoan_PyTorch_ONNX.onnx\")\n",
    "\n",
    "    def to_numpy(tensor):\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(X_test_var)} \n",
    "    ort_outs = torch.FloatTensor(ort_session.run(None, ort_inputs))\n",
    "     \n",
    "     \n",
    "    predict_proba = torch.sigmoid(ort_outs)   \n",
    "    #predict_proba = predict_proba.numpy()\n",
    "    \n",
    "\n",
    "    P_BAD = float(predict_proba[0][0])\n",
    "     \n",
    "     \n",
    "    return (P_BAD)\"\"\"\n",
    " \n",
    "f = open('Data_orig/AutoLoan_Pytorch_ONNX.py',\"w+\")\n",
    "f.write(score_code)\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 2022 Latest",
   "language": "python",
   "name": "pytorch2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
